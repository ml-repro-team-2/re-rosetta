{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7456878,"sourceType":"datasetVersion","datasetId":4340540},{"sourceId":7476374,"sourceType":"datasetVersion","datasetId":4351994},{"sourceId":7510747,"sourceType":"datasetVersion","datasetId":4331920,"isSourceIdPinned":true},{"sourceId":7532991,"sourceType":"datasetVersion","datasetId":4387417}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ml-repro-team-2/re-rosetta.git\n%cd /kaggle/working/re-rosetta/\n!git checkout aakash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf /kaggle/working/re-rosetta/matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/image-net-style-gan-xl-103/re-rosetta/matches /kaggle/working/re-rosetta -r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/image-net-style-gan-xl-103/re-rosetta/matches/styleganxl/clip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CONFIG\nCLASSID = 235","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Requirements\n!pip install -r requirements.txt\n!pip install transformers\n!pip install openai-clip\n!pip install einops\n!pip install pytorch_pretrained_biggan\n!pip install Ninja\n!pip install timm==0.4.12\n!pip install dill","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown \"https://drive.google.com/uc?export=download&id=1EX4J4Al5cGC8Z4ZPV5v576LBjLqw_Jt6\" -O 'mae_pretrain_vit_base.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL\ngan_mode = 'styleganxl'\ndiscr_mode = 'dino'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PAIRWISE MATCHING","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nimport torch\nimport torchvision\nfrom torchvision.models import resnet50\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport clip\nfrom PIL import Image\nimport requests\nimport torch.hub\nimport time\nimport pickle\nimport os\nimport numpy\nimport math\nimport match_utils\nfrom match_utils import matching, stats, proggan, nethook, dataset, models, layers, loading, visualize_pairwisematch\ndevice = torch.device('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf imgs\n\n!mkdir -p imgs/dir0\n!mkdir -p imgs/dir1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets\nval_dataset = datasets.ImageFolder(root = '/kaggle/input/imagenet-val-sample')\nfname = val_dataset.classes[CLASSID]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ip = f\"/kaggle/input/imagenet-val-sample/{fname}\"\nip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 35\ntorch.manual_seed(seed)\ndevice = torch.device(device)\n\n# Load models and tables\ndiscr, discr_layers = models.load_discr(discr_mode, device)\n\nG, gan_layers = models.load_gan(gan_mode, device)\nganlayers, discrlayers = layers.get_layers(\n    G, gan_layers, discr, discr_layers, gan_mode, discr_mode, device\n)\nG = nethook.InstrumentedModel(G)\nG.retain_layers(gan_layers, detach=False)\ndiscr = nethook.InstrumentedModel(discr)\ndiscr.retain_layers(discr_layers)\n\nall_images = {}\nfiles = os.listdir(ip)\nfor file in files:\n    file = file\n    biggan_resolution = 128\n    target_fname = f\"{ip}/{file}\"\n    target_pil = PIL.Image.open(target_fname).convert(\"RGB\")\n    w, h = target_pil.size\n    s = min(w, h)\n    target_pil = target_pil.crop(\n        ((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2)\n    )\n    target_pil = target_pil.resize(\n        (biggan_resolution, biggan_resolution), PIL.Image.LANCZOS\n    )\n    target_uint8 = np.array(target_pil, dtype=np.uint8)\n    plt.imshow(target_uint8)\n    plt.axis('off')\n    plt.savefig((f\"imgs/dir0/{file}\"[:-4] + \"png\").lower(), bbox_inches=\"tight\", pad_inches=-0.1)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom scipy.stats import truncnorm\n\n\ndef truncate_noise(size, truncation):\n    \"\"\"\n    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)\n    and truncation value, creates a tensor of that shape filled with random\n    numbers from the truncated normal distribution.\n    Parameters:\n        n_samples: the number of samples to generate, a scalar\n        z_dim: the dimension of the noise vector, a scalar\n        truncation: the truncation value, a non-negative scalar\n    \"\"\"\n\n    truncated_noise = truncnorm.rvs(-1 * truncation, truncation, size=size)\n\n    return torch.Tensor(truncated_noise)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device)\n\n\n# for i in range(1, 10):\n#     with open(f\"imgs/dist{i}.txt\", \"w\") as f:\n#         f.write(f\"{i}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k = 5\nresnet_path = f\"matches/{gan_mode}/{discr_mode}/{CLASSID}\"\n\ntable, gan_stats, discr_stats = loading.load_stats(resnet_path, device)\nclasss = CLASSID","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code = '''\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport torch\n# from torch.autograd import Variable\n\nfrom lpips.trainer import *\nfrom lpips.lpips import *\n\ndef normalize_tensor(in_feat,eps=1e-10):\n    norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1,keepdim=True))\n    return in_feat/(norm_factor+eps)\n\ndef l2(p0, p1, range=255.):\n    return .5*np.mean((p0 / range - p1 / range)**2)\n\ndef psnr(p0, p1, peak=255.):\n    return 10*np.log10(peak**2/np.mean((1.*p0-1.*p1)**2))\n\ndef dssim(p0, p1, range=255.):\n    from skimage.measure import compare_ssim\n    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.\n\ndef tensor2np(tensor_obj):\n    # change dimension of a tensor object into a numpy array\n    return tensor_obj[0].cpu().float().numpy().transpose((1,2,0))\n\ndef np2tensor(np_obj):\n     # change dimenion of np array into tensor array\n    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\ndef tensor2tensorlab(image_tensor,to_norm=True,mc_only=False):\n    # image tensor to lab tensor\n    from skimage import color\n\n    img = tensor2im(image_tensor)\n    img_lab = color.rgb2lab(img)\n    if(mc_only):\n        img_lab[:,:,0] = img_lab[:,:,0]-50\n    if(to_norm and not mc_only):\n        img_lab[:,:,0] = img_lab[:,:,0]-50\n        img_lab = img_lab/100.\n\n    return np2tensor(img_lab)\n\ndef tensorlab2tensor(lab_tensor,return_inbnd=False):\n    from skimage import color\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n\n    lab = tensor2np(lab_tensor)*100.\n    lab[:,:,0] = lab[:,:,0]+50\n\n    rgb_back = 255.*np.clip(color.lab2rgb(lab.astype('float')),0,1)\n    if(return_inbnd):\n        # convert back to lab, see if we match\n        lab_back = color.rgb2lab(rgb_back.astype('uint8'))\n        mask = 1.*np.isclose(lab_back,lab,atol=2.)\n        mask = np2tensor(np.prod(mask,axis=2)[:,:,np.newaxis])\n        return (im2tensor(rgb_back),mask)\n    else:\n        return im2tensor(rgb_back)\n\ndef load_image(path):\n    if(path[-3:] == 'dng'):\n        import rawpy\n        with rawpy.imread(path) as raw:\n            img = raw.postprocess()\n    elif(path[-3:]=='bmp' or path[-3:]=='jpg' or path[-3:]=='png' or path[-4:]=='jpeg'):\n        import cv2\n        return cv2.imread(path)[:,:,::-1]\n    else:\n        import matplotlib.pyplot as plt        \n        img = (255*plt.imread(path)[:,:,:3]).astype('uint8')\n\n    return img\n\ndef tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n    return image_numpy.astype(imtype)\n\ndef im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n    return torch.Tensor((image / factor - cent)\n                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\ndef tensor2vec(vector_tensor):\n    return vector_tensor.data.cpu().numpy()[:, :, 0, 0]\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    \"\"\"\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap'''","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lpips","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/opt/conda/lib/python3.10/site-packages/lpips/__init__.py\",\"w\") as f:\n    f.write(code)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lpips","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_steps = 300\nlr_rampdown_length = 0.25\nlr_rampup_length = 0.05\ninitial_learning_rate = 0.01\nSHOW_IMAGES = False\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l2_criterion = torch.nn.MSELoss(reduction='mean')\nlpips_fn = lpips.LPIPS(net='vgg', version='0.1').cuda()\n\nfor k in [5,]:\n    gan_matches = torch.argmax(table, 1)\n    _, discr_matches = torch.topk(table, k=k, dim=0)\n\n    ##get best buddies\n    perfect_matches = []\n    discr_perfect_matches = []\n    num_perfect_matches = 0\n    for i in range(table.shape[0]):\n        gan_match = gan_matches[i].item()\n        discr_match = discr_matches[:, gan_match]\n        if discr_match.ndim == 1:\n            if i in discr_match:\n                num_perfect_matches += 1\n                perfect_matches.append(i)\n                discr_perfect_matches.append(gan_match)\n        else:\n            if i == discr_match:\n                num_perfect_matches += 1\n                perfect_matches.append(i)\n                discr_perfect_matches.append(gan_match)\n    #     getting everything in (layer, unit) form\n    for i, unit in enumerate(perfect_matches):\n        perfect_matches[i] = layers.find_act(perfect_matches[i], ganlayers)\n    for i, unit in enumerate(discr_perfect_matches):\n        discr_perfect_matches[i] = layers.find_act(discr_perfect_matches[i], discrlayers)\n        \n    all_images = {}\n    biggan_resolution = 128\n    files = os.listdir(ip)\n    for file in files:\n        target_fname = f\"{ip}/{file}\"\n        discr_im = Image.open(target_fname).convert(\"RGB\")\n        discr_im_np = discr_im\n        discr_im = torchvision.transforms.ToTensor()(discr_im).unsqueeze(0).to(device)\n\n        discr_im = torch.nn.functional.interpolate(discr_im, size=(224, 224), mode=\"bicubic\")\n        discr_im = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(discr_im)\n        if SHOW_IMAGES:\n            plt.imshow(discr_im.squeeze().permute(1,2,0).cpu())\n            plt.show()\n\n        logits = discr(discr_im)\n        assert(CLASSID, torch.argmax(logits))\n        discr_activs = matching.store_activs(discr, discr_layers)\n\n        # normalize\n        eps = 0.00001\n        for i, _ in enumerate(discr_activs):\n            discr_activs[i] = (discr_activs[i] - discr_stats[i][0]) / (discr_stats[i][1] + eps)\n\n        discr_perfect_activs = []\n        for idx in discr_perfect_matches:\n            discr_perfect_activs.append(discr_activs[idx[0]][:, idx[1], :, :].unsqueeze(0))\n\n        #     target_images = target.unsqueeze(0).to(device).to(torch.float32)\n        z = truncate_noise((1, 64), 1).to(device)\n        c = torch.zeros((1, 1000)).to(device)\n        c[0, classs] = 1\n        im = G(z, c, 1)\n        im = im[0].permute((1, 2, 0))\n        im = (im + 1) / 2\n        print(f\"{CLASSID} : {file} , initially generated image\")\n        if SHOW_IMAGES:\n            plt.imshow(im.detach().cpu())\n            plt.title('initially generated image')\n            plt.show()\n        z1 = torch.tensor(\n            truncate_noise((1, 64), 1),\n            dtype=torch.float32,\n            device=device,\n            requires_grad=True,).to(device)  # pylint: disable=not-callable\n        optimizer = torch.optim.Adam([z1], betas=(0.9, 0.999), lr=initial_learning_rate)\n        # @title Inversion\n        # all_images = []\n        for step in range(num_steps):\n            # Learning rate schedule.\n            t = step / num_steps\n            lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n            lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n            lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n            lr = initial_learning_rate * lr_ramp\n            for param_group in optimizer.param_groups:\n                param_group[\"lr\"] = lr\n\n            # Synth images from opt_w.\n            synth_images = G(z1, c, 1)\n\n            # track images\n            synth_images = (synth_images + 1) * (255 / 2)\n            synth_images_np = (\n                synth_images.clone()\n                .detach()\n                .permute(0, 2, 3, 1)\n                .clamp(0, 255)\n                .to(torch.uint8)[0]\n                .cpu()\n                .numpy()\n            )\n            #     all_images.append(synth_images_np)\n\n            # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n            if synth_images.shape[2] > 256:\n                synth_images = F.interpolate(synth_images, size=(256, 256), mode=\"area\")\n\n            gan_activs = matching.store_activs(G, gan_layers)\n            # normalize all activations\n            eps = 0.00001\n            for i, _ in enumerate(gan_activs):\n                gan_activs[i] = (gan_activs[i] - gan_stats[i][0]) / (gan_stats[i][1] + eps)\n\n            gan_perfect_activs = []\n            for idx in perfect_matches:\n                gan_perfect_activs.append(gan_activs[idx[0]][:, idx[1], :, :])\n\n            # pearson correlation\n            a_loss = 0\n            counter = len(gan_perfect_activs)\n            for i, _ in enumerate(gan_perfect_activs):\n                map_size = max((gan_perfect_activs[i].shape[1], discr_perfect_activs[i].shape[1]))\n                gan_activ_new = torch.nn.Upsample(size=(map_size, map_size), mode=\"bilinear\")(gan_perfect_activs[i].unsqueeze(0))\n                discr_activ_new = torch.nn.Upsample(size=(map_size, map_size), mode=\"bilinear\")(discr_perfect_activs[i])\n                prod = torch.einsum(\"aixy,ajxy->ij\", gan_activ_new, discr_activ_new)\n                div1 = torch.sum(gan_activ_new**2)\n                div2 = torch.sum(discr_activ_new**2)\n                corr = prod / torch.sqrt(div1 * div2)\n                a_loss += corr\n\n            a_loss *= -1\n            a_loss = a_loss / counter\n            l_reg = torch.mean((z1 - z) ** 2)\n            \n            # perceptual loss\n            mse_loss = l2_criterion(synth_images, F.interpolate(discr_im, size=(128, 128), mode='area'))\n            lpips_loss = lpips_fn.forward(synth_images, F.interpolate(discr_im, size=(128, 128), mode='area'))\n            \n            # Features for synth images.\n            loss = lpips_loss\n#             loss = a_loss\n            \n            # Step\n            optimizer.zero_grad(set_to_none=True)\n\n            loss.backward()\n            optimizer.step()\n            msg  = f'[ step {step+1:>4d}/{num_steps}] '\n#             msg += f'[ mse_loss: {float(mse_loss):<5.2f} lpips_loss: {float(lpips_loss):<5.4f}] '\n            msg += f'[ a_loss: {float(a_loss):<5.2f} loss_reg: {0.1 * float(l_reg):<5.2f}] '\n            if (step+1) % 50 == 0 or step == 0:\n                print(msg) \n                if SHOW_IMAGES:\n                    plt.imshow(synth_images_np)\n                    plt.axis(\"off\")\n                    plt.show()\n            if step == num_steps - 1:\n                print(msg)\n                plt.imshow(synth_images_np)\n                plt.axis(\"off\")\n                file = file[:-4] + \"png\"\n                print(f\"inverted image - {CLASSID}:{file}\")\n                plt.savefig(f\"imgs/dir1/{file}\".lower(), bbox_inches=\"tight\", pad_inches=-0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"command = f\"python calc_metrics.py --classid {CLASSID} --type dino --dir0 imgs/dir0 --dir1 imgs/dir1 -o imgs/dist.csv --use_gpu --version 0.1\"\nos.system(command)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display inversion results (images)\nfor root, _, files in os.walk('imgs/dir1'):    \n    for img_path in files:\n        fig, axes = plt.subplots(1, 2)\n        img1 = Image.open(os.path.join('imgs/dir0', img_path))\n        img2 = Image.open(os.path.join('imgs/dir1', img_path))\n        axes[0].imshow(img1)\n        axes[0].set_title('target image')\n        axes[0].set_axis_off()\n        axes[1].imshow(img2)\n        axes[1].set_title('inverted image')\n        axes[1].set_axis_off()\n        fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r inversion_results.zip imgs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/re-rosetta","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
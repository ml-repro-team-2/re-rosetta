{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T18:58:54.693680Z","iopub.status.busy":"2024-01-21T18:58:54.692995Z","iopub.status.idle":"2024-01-21T18:58:55.093583Z","shell.execute_reply":"2024-01-21T18:58:55.092650Z","shell.execute_reply.started":"2024-01-21T18:58:54.693635Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T18:58:55.096240Z","iopub.status.busy":"2024-01-21T18:58:55.095645Z","iopub.status.idle":"2024-01-21T18:58:55.101956Z","shell.execute_reply":"2024-01-21T18:58:55.100808Z","shell.execute_reply.started":"2024-01-21T18:58:55.096198Z"},"trusted":true},"outputs":[],"source":["## CONFIG\n","classid = 812"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T18:58:55.104165Z","iopub.status.busy":"2024-01-21T18:58:55.103533Z","iopub.status.idle":"2024-01-21T18:58:55.110803Z","shell.execute_reply":"2024-01-21T18:58:55.110013Z","shell.execute_reply.started":"2024-01-21T18:58:55.104126Z"},"trusted":true},"outputs":[],"source":["# !git clone https://github.com/ml-repro-team-2/re-rosetta.git\n","# !git checkout aakash"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T18:58:55.113533Z","iopub.status.busy":"2024-01-21T18:58:55.112895Z","iopub.status.idle":"2024-01-21T18:58:55.122745Z","shell.execute_reply":"2024-01-21T18:58:55.121487Z","shell.execute_reply.started":"2024-01-21T18:58:55.113495Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/re-rosetta\n"]}],"source":["%cd /kaggle/working/re-rosetta/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:42:53.904976Z","iopub.status.busy":"2024-01-21T16:42:53.904593Z","iopub.status.idle":"2024-01-21T16:42:53.910671Z","shell.execute_reply":"2024-01-21T16:42:53.909666Z","shell.execute_reply.started":"2024-01-21T16:42:53.904945Z"},"trusted":true},"outputs":[],"source":["!cp /kaggle/input/class-949/re-rosetta/matches /kaggle/working/re-rosetta -r"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"collapsed":true,"execution":{"iopub.execute_input":"2024-01-21T16:42:53.914221Z","iopub.status.busy":"2024-01-21T16:42:53.913891Z","iopub.status.idle":"2024-01-21T16:45:34.745870Z","shell.execute_reply":"2024-01-21T16:45:34.744729Z","shell.execute_reply.started":"2024-01-21T16:42:53.914195Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["#@title Requirements\n","!pip install -r requirements.txt\n","!pip install transformers\n","!pip install openai-clip\n","!pip install einops\n","!pip install pytorch_pretrained_biggan\n","!pip install Ninja\n","!pip install timm==0.4.12\n","!pip install dill"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!gdown \"https://drive.google.com/uc?export=download&id=1EX4J4Al5cGC8Z4ZPV5v576LBjLqw_Jt6\" -O 'mae_pretrain_vit_base.pth'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:58:42.011994Z","iopub.status.busy":"2024-01-21T16:58:42.011623Z","iopub.status.idle":"2024-01-21T16:58:42.016721Z","shell.execute_reply":"2024-01-21T16:58:42.015621Z","shell.execute_reply.started":"2024-01-21T16:58:42.011967Z"},"trusted":true},"outputs":[],"source":["import PIL\n","gan_mode = 'styleganxl'\n","# idx = 207#golden retriever\n","# for discrmode in ['resnet50','clip','dino','mae','dino_vitb8','dino_vitb16']:\n","#     save = f\"matches/{ganmode}/{discrmode}/{classid}\"\n","#     !python match.py --device \"cuda:0\" --save_path {save}  --gan_mode {ganmode} --discr_mode {discrmode} --batch_size 2 --epochs 800 --classidx {classid}\n","\n","discr_mode = 'resnet50'"]},{"cell_type":"markdown","metadata":{},"source":["## PAIRWISE MATCHING"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:56:04.978475Z","iopub.status.busy":"2024-01-21T16:56:04.978130Z","iopub.status.idle":"2024-01-21T16:56:04.984973Z","shell.execute_reply":"2024-01-21T16:56:04.983985Z","shell.execute_reply.started":"2024-01-21T16:56:04.978446Z"},"trusted":true},"outputs":[],"source":["from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import torchvision\n","from torchvision.models import resnet50\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import clip\n","from PIL import Image\n","import requests\n","import torch.hub\n","import time\n","import pickle\n","import os\n","import math\n","import match_utils\n","from match_utils import matching, stats, proggan, nethook, dataset, models, layers, loading, visualize_pairwisematch\n","device = torch.device('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:56:05.288102Z","iopub.status.busy":"2024-01-21T16:56:05.287816Z","iopub.status.idle":"2024-01-21T16:56:05.292087Z","shell.execute_reply":"2024-01-21T16:56:05.291120Z","shell.execute_reply.started":"2024-01-21T16:56:05.288079Z"},"trusted":true},"outputs":[],"source":["!mkdir -p imgs/dir0\n","!mkdir -p imgs/dir1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:56:05.692611Z","iopub.status.busy":"2024-01-21T16:56:05.692288Z","iopub.status.idle":"2024-01-21T16:56:06.106928Z","shell.execute_reply":"2024-01-21T16:56:06.105991Z","shell.execute_reply.started":"2024-01-21T16:56:05.692585Z"},"trusted":true},"outputs":[],"source":["from torchvision import datasets\n","val_dataset = datasets.ImageFolder(root = '/kaggle/input/imagenet-val-mini/imagenet-val-mini')\n","fname = val_dataset.classes[classid]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:56:06.251843Z","iopub.status.busy":"2024-01-21T16:56:06.251438Z","iopub.status.idle":"2024-01-21T16:56:06.257504Z","shell.execute_reply":"2024-01-21T16:56:06.256576Z","shell.execute_reply.started":"2024-01-21T16:56:06.251814Z"},"trusted":true},"outputs":[],"source":["ip = f\"/kaggle/input/imagenet-val-mini/imagenet-val-mini/{fname}\"\n","ip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T16:58:51.910229Z","iopub.status.busy":"2024-01-21T16:58:51.909844Z","iopub.status.idle":"2024-01-21T17:05:47.343033Z","shell.execute_reply":"2024-01-21T17:05:47.342077Z","shell.execute_reply.started":"2024-01-21T16:58:51.910199Z"},"trusted":true},"outputs":[],"source":["num_steps = 200\n","lr_rampdown_length = 0.25\n","lr_rampup_length = 0.05\n","initial_learning_rate = 0.01\n","seed = 35\n","torch.manual_seed(seed)\n","device = torch.device(device)\n","\n","# Load models and tables\n","discr, discr_layers = models.load_discr(discr_mode, device)\n","\n","G, gan_layers = models.load_gan(gan_mode, device)\n","ganlayers, discrlayers = layers.get_layers(\n","    G, gan_layers, discr, discr_layers, gan_mode, discr_mode, device\n",")\n","G = nethook.InstrumentedModel(G)\n","G.retain_layers(gan_layers, detach=False)\n","discr = nethook.InstrumentedModel(discr)\n","discr.retain_layers(discr_layers)\n","\n","all_images = {}\n","files = os.listdir(ip)\n","for file in files:\n","    file = file\n","    biggan_resolution = 128\n","    target_fname = f\"{ip}/{file}\"\n","    target_pil = PIL.Image.open(target_fname).convert(\"RGB\")\n","    w, h = target_pil.size\n","    s = min(w, h)\n","    target_pil = target_pil.crop(\n","        ((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2)\n","    )\n","    target_pil = target_pil.resize(\n","        (biggan_resolution, biggan_resolution), PIL.Image.LANCZOS\n","    )\n","    target_uint8 = np.array(target_pil, dtype=np.uint8)\n","    plt.imshow(target_uint8)\n","    plt.savefig((f\"imgs/dir0/{file}\"[:-4] + \"png\").lower())\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:05:47.344900Z","iopub.status.busy":"2024-01-21T17:05:47.344583Z","iopub.status.idle":"2024-01-21T17:05:47.352570Z","shell.execute_reply":"2024-01-21T17:05:47.349651Z","shell.execute_reply.started":"2024-01-21T17:05:47.344875Z"},"trusted":true},"outputs":[],"source":["\n","from scipy.stats import truncnorm\n","\n","\n","def truncate_noise(size, truncation):\n","    \"\"\"\n","    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)\n","    and truncation value, creates a tensor of that shape filled with random\n","    numbers from the truncated normal distribution.\n","    Parameters:\n","        n_samples: the number of samples to generate, a scalar\n","        z_dim: the dimension of the noise vector, a scalar\n","        truncation: the truncation value, a non-negative scalar\n","    \"\"\"\n","\n","    truncated_noise = truncnorm.rvs(-1 * truncation, truncation, size=size)\n","\n","    return torch.Tensor(truncated_noise)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:05:47.354047Z","iopub.status.busy":"2024-01-21T17:05:47.353775Z","iopub.status.idle":"2024-01-21T17:05:47.388406Z","shell.execute_reply":"2024-01-21T17:05:47.387563Z","shell.execute_reply.started":"2024-01-21T17:05:47.354025Z"},"trusted":true},"outputs":[],"source":["\n","# target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device)\n","\n","\n","for i in range(1, 10):\n","    with open(f\"imgs/dist{i}.txt\", \"w\") as f:\n","        f.write(f\"{i}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:05:47.391144Z","iopub.status.busy":"2024-01-21T17:05:47.390774Z","iopub.status.idle":"2024-01-21T17:05:48.314465Z","shell.execute_reply":"2024-01-21T17:05:48.313532Z","shell.execute_reply.started":"2024-01-21T17:05:47.391113Z"},"trusted":true},"outputs":[],"source":["# k = 5\n","resnet_path = f\"matches/{gan_mode}/{discr_mode}/{classid}\"\n","\n","table, gan_stats, discr_stats = loading.load_stats(resnet_path, device)\n","classs = classid"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:05:48.316633Z","iopub.status.busy":"2024-01-21T17:05:48.316067Z","iopub.status.idle":"2024-01-21T17:05:48.326023Z","shell.execute_reply":"2024-01-21T17:05:48.324997Z","shell.execute_reply.started":"2024-01-21T17:05:48.316598Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["code = '''\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import numpy as np\n","import torch\n","# from torch.autograd import Variable\n","\n","from lpips.trainer import *\n","from lpips.lpips import *\n","\n","def normalize_tensor(in_feat,eps=1e-10):\n","    norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1,keepdim=True))\n","    return in_feat/(norm_factor+eps)\n","\n","def l2(p0, p1, range=255.):\n","    return .5*np.mean((p0 / range - p1 / range)**2)\n","\n","def psnr(p0, p1, peak=255.):\n","    return 10*np.log10(peak**2/np.mean((1.*p0-1.*p1)**2))\n","\n","def dssim(p0, p1, range=255.):\n","    from skimage.measure import compare_ssim\n","    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.\n","\n","def tensor2np(tensor_obj):\n","    # change dimension of a tensor object into a numpy array\n","    return tensor_obj[0].cpu().float().numpy().transpose((1,2,0))\n","\n","def np2tensor(np_obj):\n","     # change dimenion of np array into tensor array\n","    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n","\n","def tensor2tensorlab(image_tensor,to_norm=True,mc_only=False):\n","    # image tensor to lab tensor\n","    from skimage import color\n","\n","    img = tensor2im(image_tensor)\n","    img_lab = color.rgb2lab(img)\n","    if(mc_only):\n","        img_lab[:,:,0] = img_lab[:,:,0]-50\n","    if(to_norm and not mc_only):\n","        img_lab[:,:,0] = img_lab[:,:,0]-50\n","        img_lab = img_lab/100.\n","\n","    return np2tensor(img_lab)\n","\n","def tensorlab2tensor(lab_tensor,return_inbnd=False):\n","    from skimage import color\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","\n","    lab = tensor2np(lab_tensor)*100.\n","    lab[:,:,0] = lab[:,:,0]+50\n","\n","    rgb_back = 255.*np.clip(color.lab2rgb(lab.astype('float')),0,1)\n","    if(return_inbnd):\n","        # convert back to lab, see if we match\n","        lab_back = color.rgb2lab(rgb_back.astype('uint8'))\n","        mask = 1.*np.isclose(lab_back,lab,atol=2.)\n","        mask = np2tensor(np.prod(mask,axis=2)[:,:,np.newaxis])\n","        return (im2tensor(rgb_back),mask)\n","    else:\n","        return im2tensor(rgb_back)\n","\n","def load_image(path):\n","    if(path[-3:] == 'dng'):\n","        import rawpy\n","        with rawpy.imread(path) as raw:\n","            img = raw.postprocess()\n","    elif(path[-3:]=='bmp' or path[-3:]=='jpg' or path[-3:]=='png' or path[-4:]=='jpeg'):\n","        import cv2\n","        return cv2.imread(path)[:,:,::-1]\n","    else:\n","        import matplotlib.pyplot as plt        \n","        img = (255*plt.imread(path)[:,:,:3]).astype('uint8')\n","\n","    return img\n","\n","def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n","    image_numpy = image_tensor[0].cpu().float().numpy()\n","    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n","    return image_numpy.astype(imtype)\n","\n","def im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n","    return torch.Tensor((image / factor - cent)\n","                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n","\n","def tensor2vec(vector_tensor):\n","    return vector_tensor.data.cpu().numpy()[:, :, 0, 0]\n","\n","\n","def voc_ap(rec, prec, use_07_metric=False):\n","    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n","    Compute VOC AP given precision and recall.\n","    If use_07_metric is true, uses the\n","    VOC 07 11 point method (default:False).\n","    \"\"\"\n","    if use_07_metric:\n","        # 11 point metric\n","        ap = 0.\n","        for t in np.arange(0., 1.1, 0.1):\n","            if np.sum(rec >= t) == 0:\n","                p = 0\n","            else:\n","                p = np.max(prec[rec >= t])\n","            ap = ap + p / 11.\n","    else:\n","        # correct AP calculation\n","        # first append sentinel values at the end\n","        mrec = np.concatenate(([0.], rec, [1.]))\n","        mpre = np.concatenate(([0.], prec, [0.]))\n","\n","        # compute the precision envelope\n","        for i in range(mpre.size - 1, 0, -1):\n","            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n","\n","        # to calculate area under PR curve, look for points\n","        # where X axis (recall) changes value\n","        i = np.where(mrec[1:] != mrec[:-1])[0]\n","\n","        # and sum (\\Delta recall) * prec\n","        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n","    return ap'''"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!pip install lpips"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:06:18.475385Z","iopub.status.busy":"2024-01-21T17:06:18.474472Z","iopub.status.idle":"2024-01-21T17:06:18.480680Z","shell.execute_reply":"2024-01-21T17:06:18.479559Z","shell.execute_reply.started":"2024-01-21T17:06:18.475347Z"},"trusted":true},"outputs":[],"source":["with open(\"/opt/conda/lib/python3.10/site-packages/lpips/__init__.py\",\"w\") as f:\n","    f.write(code)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:06:19.791616Z","iopub.status.busy":"2024-01-21T17:06:19.791252Z","iopub.status.idle":"2024-01-21T17:06:19.804471Z","shell.execute_reply":"2024-01-21T17:06:19.803635Z","shell.execute_reply.started":"2024-01-21T17:06:19.791587Z"},"trusted":true},"outputs":[],"source":["import lpips"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-21T17:58:59.379893Z","iopub.status.busy":"2024-01-21T17:58:59.379533Z","iopub.status.idle":"2024-01-21T18:10:14.054976Z","shell.execute_reply":"2024-01-21T18:10:14.053969Z","shell.execute_reply.started":"2024-01-21T17:58:59.379866Z"},"trusted":true},"outputs":[],"source":["for k in range(1, 2):\n","    gan_matches = torch.argmax(table, 1)\n","    _, discr_matches = torch.topk(table, k=k, dim=0)\n","\n","    ##get best buddies\n","    perfect_matches = []\n","    discr_perfect_matches = []\n","    num_perfect_matches = 0\n","    for i in range(table.shape[0]):\n","        gan_match = gan_matches[i].item()\n","        discr_match = discr_matches[:, gan_match]\n","        if discr_match.ndim == 1:\n","            if i in discr_match:\n","                num_perfect_matches += 1\n","                perfect_matches.append(i)\n","                discr_perfect_matches.append(gan_match)\n","        else:\n","            if i == discr_match:\n","                num_perfect_matches += 1\n","                perfect_matches.append(i)\n","                discr_perfect_matches.append(gan_match)\n","    #     getting everything in (layer, unit) form\n","    for i, unit in enumerate(perfect_matches):\n","        perfect_matches[i] = layers.find_act(perfect_matches[i], ganlayers)\n","    for i, unit in enumerate(discr_perfect_matches):\n","        discr_perfect_matches[i] = layers.find_act(\n","            discr_perfect_matches[i], discrlayers\n","        )\n","    all_images = {}\n","    biggan_resolution = 128\n","    files = os.listdir(ip)\n","    for file in files:\n","        target_fname = f\"{ip}/{file}\"\n","        discr_im = Image.open(target_fname).convert(\"RGB\")\n","        discr_im = torchvision.transforms.ToTensor()(discr_im).unsqueeze(0).to(device)\n","\n","        discr_im = torch.nn.functional.interpolate(\n","            discr_im, size=(244, 244), mode=\"bicubic\"\n","        )\n","        discr_im = torchvision.transforms.Normalize(\n","            (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n","        )(discr_im)\n","        #         plt.imshow(discr_im.squeeze().permute(1,2,0).cpu())\n","        #         plt.show()\n","\n","        logits = discr(discr_im)\n","        assert(classid, torch.argmax(logits))\n","        discr_activs = matching.store_activs(discr, discr_layers)\n","\n","        # normalize\n","        eps = 0.00001\n","        for i, _ in enumerate(discr_activs):\n","            discr_activs[i] = (discr_activs[i] - discr_stats[i][0]) / (\n","                discr_stats[i][1] + eps\n","            )\n","\n","        discr_perfect_activs = []\n","        for idx in discr_perfect_matches:\n","            discr_perfect_activs.append(\n","                discr_activs[idx[0]][:, idx[1], :, :].unsqueeze(0)\n","            )\n","\n","        #     target_images = target.unsqueeze(0).to(device).to(torch.float32)\n","        z = truncate_noise((1, 64), 1).to(device)\n","        c = torch.zeros((1, 1000)).to(device)\n","        c[0, classs] = 1\n","        im = G(z, c, 1)\n","        im = im[0].permute((1, 2, 0))\n","        im = (im + 1) / 2\n","        print(f\"{classid} : {file} , original generated image\")\n","        plt.imshow(im.detach().cpu())\n","        plt.title('original generated image')\n","        plt.show()\n","        z1 = torch.tensor(\n","            truncate_noise((1, 64), 1),\n","            dtype=torch.float32,\n","            device=device,\n","            requires_grad=True,\n","        ).to(\n","            device\n","        )  # pylint: disable=not-callable\n","        optimizer = torch.optim.Adam([z1], betas=(0.9, 0.999), lr=initial_learning_rate)\n","        # @title Inversion\n","        # all_images = []\n","        for step in range(num_steps):\n","            # Learning rate schedule.\n","            t = step / num_steps\n","            lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n","            lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n","            lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n","            lr = initial_learning_rate * lr_ramp\n","            for param_group in optimizer.param_groups:\n","                param_group[\"lr\"] = lr\n","\n","            # Synth images from opt_w.\n","            synth_images = G(z1, c, 1)\n","\n","            # track images\n","            synth_images = (synth_images + 1) * (255 / 2)\n","            synth_images_np = (\n","                synth_images.clone()\n","                .detach()\n","                .permute(0, 2, 3, 1)\n","                .clamp(0, 255)\n","                .to(torch.uint8)[0]\n","                .cpu()\n","                .numpy()\n","            )\n","            #     all_images.append(synth_images_np)\n","\n","            # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n","            if synth_images.shape[2] > 256:\n","                synth_images = F.interpolate(synth_images, size=(256, 256), mode=\"area\")\n","\n","            gan_activs = matching.store_activs(G, gan_layers)\n","            # normalize all activations\n","            eps = 0.00001\n","            for i, _ in enumerate(gan_activs):\n","                gan_activs[i] = (gan_activs[i] - gan_stats[i][0]) / (\n","                    gan_stats[i][1] + eps\n","                )\n","\n","            gan_perfect_activs = []\n","            for idx in perfect_matches:\n","                gan_perfect_activs.append(gan_activs[idx[0]][:, idx[1], :, :])\n","\n","            # pearson correlation\n","            a_loss = 0\n","            counter = len(gan_perfect_activs)\n","            for i, _ in enumerate(gan_perfect_activs):\n","                map_size = max(\n","                    (gan_perfect_activs[i].shape[1], discr_perfect_activs[i].shape[1])\n","                )\n","                gan_activ_new = torch.nn.Upsample(\n","                    size=(map_size, map_size), mode=\"bilinear\"\n","                )(gan_perfect_activs[i].unsqueeze(0))\n","                discr_activ_new = torch.nn.Upsample(\n","                    size=(map_size, map_size), mode=\"bilinear\"\n","                )(discr_perfect_activs[i])\n","                prod = torch.einsum(\"aixy,ajxy->ij\", gan_activ_new, discr_activ_new)\n","                div1 = torch.sum(gan_activ_new**2)\n","                div2 = torch.sum(discr_activ_new**2)\n","                corr = prod / torch.sqrt(div1 * div2)\n","                a_loss += corr\n","\n","            a_loss *= -1\n","            a_loss = a_loss / counter\n","            l_reg = torch.mean((z1 - z) ** 2)\n","            \n","            # perceptual loss\n","            assert(synth_images.shape == discr_im.shape)\n","            mse_loss = torch.nn.MSELoss(synth_images, discr_im, reduction='mean', device=device)\n","            \n","            # Features for synth images.\n","            loss = a_loss + 0 * l_reg\n","            # Step\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            loss.backward()\n","            optimizer.step()\n","            msg  = f'[ step {step+1:>4d}/{num_steps}] '\n","            msg += f'[ a_loss: {float(a_loss):5.2f} loss_reg: {0.1 * float(l_reg):5.2f}] '\n","            print(msg)\n","            if step == num_steps - 1:\n","                plt.imshow(synth_images_np)\n","                plt.axis(\"off\")\n","                file = file[:-4] + \"png\"\n","                print(f\"inverted image - {classid}:{file}\")\n","                plt.show()\n","                plt.savefig(f\"imgs/dir1/{file}\".lower())\n","                #         if step % 50 == 0:\n","                #             plt.imshow(synth_images_np)\n","                #             plt.show()\n","                command = f\"python calc_metrics.py  --dir0 imgs/dir0 --dir1 imgs/dir1 -o imgs/dist{k}.txt --use_gpu --version 0.1\"\n","                os.system(command)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":547506,"sourceId":998277,"sourceType":"datasetVersion"},{"datasetId":4329718,"isSourceIdPinned":true,"sourceId":7439242,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
